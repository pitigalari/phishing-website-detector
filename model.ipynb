{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoIFhIfbIvOIfw26PItDba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshsoftco/phishing-website-detector/blob/1.2/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfTgXsdqqB8v"
      },
      "source": [
        "**Desicion Tree based**\n",
        "#**Phishing Website Detector**\n",
        "\n",
        "---\n",
        "\n",
        "Phishing websites are websites created to be very similar to the genuine websites of credible companies. Since these are equivalent to the legitimate ones, users are easily defrauded by them. Phishing attacks usually start with an email that asks users to update or validate their information by visiting the link given. When users click on such links they are redirected to phishing websites where they are asked to enter sensitive personal information such as usernames, passwords or credit card and bank account information. This helps attackers to carry out financial frauds eventually with the stolen information. With the number of phishing websites on the rise, it has become a serious problem even for experienced users to protect their valuable information from fraudulent persons.\n",
        "\n",
        "The basic idea of creating a phishing detector is to train a model to identify a special set of characteristics of websites by the URL of that particular site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYTcf_SkxWy9"
      },
      "source": [
        "**Libraries**\n",
        "\n",
        "Importing necessary libraries into the project. The usage of the library is mentioned below as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usae_8WZw-3H"
      },
      "source": [
        "import time # calculate performance\n",
        "import pandas as pd # read csv files / calculate correlation\n",
        "import matplotlib.pyplot as plt # draw graphs\n",
        "import seaborn as sns # draw graphs\n",
        "from sklearn import tree\n",
        "import graphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfIgBUr8TgzB"
      },
      "source": [
        "# Data Splitting and Preprocessing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLAIs9pWSarg"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "The dataset was obtained from the UCI Machine Learning Repository. It is a complete numerical dataset with over 11,000 instances.\n",
        "\n",
        "Dataset contains 30 attributes. Each row has 31 values, 30 attributes and the class.\n",
        "\n",
        "The entire dataset is separated into 3 categories as the **training set**, **validation set** and **testing set** as 75%, 20% and 5% (approx.) respectively and saved them into separate CSV files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJEBmPuAvCZl"
      },
      "source": [
        "raw_dataset_url = \"https://raw.githubusercontent.com/roshsoftco/phishing-website-detector/1.2/Dataset/raw_dataset.csv\"\n",
        "training_dataset_url = \"https://raw.githubusercontent.com/roshsoftco/phishing-website-detector/1.2/Dataset/training_dataset.csv\"\n",
        "validation_dataset_url = \"https://raw.githubusercontent.com/roshsoftco/phishing-website-detector/1.2/Dataset/validation_dataset.csv\"\n",
        "test_dataset_url = \"https://raw.githubusercontent.com/roshsoftco/phishing-website-detector/1.2/Dataset/test_dateset.csv\"\n",
        "\n",
        "## Raw Data\n",
        "raw_data = pd.read_csv(raw_dataset_url);\n",
        "\n",
        "attributes = [\n",
        "              \"having_IP_Address\", \"URL_Length\", \"Shortining_Service\", \"having_At_Symbol\", \"double_slash_redirecting\", \"Prefix_Suffix\", \"having_Sub_Domain\", \"SSLfinal_State\", \"Domain_registeration_length\", \"Favicon\", \"port\", \"HTTPS_token\",\n",
        "              \"Request_URL\", \"URL_of_Anchor\", \"Links_in_tags\", \"SFH\", \"Submitting_to_email\", \"Abnormal_URL\",\n",
        "              \"Redirect\", \"on_mouseover\", \"RightClick\", \"popUpWidnow\", \"Iframe\",\n",
        "              \"age_of_domain\", \"DNSRecord\", \"web_traffic\", \"Page_Rank\", \"Google_Index\", \"Links_pointing_to_page\", \"Statistical_report\"\n",
        "]\n",
        "\n",
        "# rm = [3,5,6,9,12,13,15,18,20,21,23,25,26,27,28,29] # attributes to remove (experiment)\n",
        "rm = None\n",
        "\n",
        "if rm is not None:\n",
        "  for ele in sorted(rm, reverse = True):  \n",
        "    del attributes[ele] # remove element from row\n",
        "\n",
        "labels = [\"Legitamate\", \"Phishing\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LfrUo8DwjgZ"
      },
      "source": [
        "**Pearson's Correlation**\n",
        "\n",
        "Following is the heatmap demonstration of the correlation calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai7ROPby1mXB"
      },
      "source": [
        "plt.figure(figsize=(24,15))\n",
        "df = pd.DataFrame(raw_data.values, columns = raw_data.columns.tolist())\n",
        "cor = df.corr()\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAmJdXJH2A3D",
        "outputId": "9cd4685e-e933-486a-e16e-1028e14ca1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Correlation with output variable\n",
        "cor_target = abs(cor[\"Result\"])\n",
        "\n",
        "#Selecting highly correlated features\n",
        "relevant_features = cor_target[cor_target>=0.1]\n",
        "print(relevant_features)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prefix_Suffix                  0.348606\n",
            "having_Sub_Domain              0.298323\n",
            "SSLfinal_State                 0.714741\n",
            "Domain_registeration_length    0.225789\n",
            "Request_URL                    0.253372\n",
            "URL_of_Anchor                  0.692935\n",
            "Links_in_tags                  0.248229\n",
            "SFH                            0.221419\n",
            "age_of_domain                  0.121496\n",
            "web_traffic                    0.346103\n",
            "Page_Rank                      0.104645\n",
            "Google_Index                   0.128950\n",
            "Result                         1.000000\n",
            "Name: Result, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzqAR9DmvFzU"
      },
      "source": [
        "**Spliting Class Labels**\n",
        "\n",
        "The `labelSplitter` function will split the last value from each row, and return them as an array to be used as class values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU-f4tmmQnUG"
      },
      "source": [
        "def labelSplitter(csv):\n",
        "  \n",
        "  data = []\n",
        "  labels = []\n",
        "  for rowArray in csv.values: # iterate through dataset row by row\n",
        "    row = rowArray.tolist()\n",
        "    labels.append(row[-1]) # extract last value as class [a1, a2, ..., class]\n",
        "    \n",
        "    if rm is not None:\n",
        "      for ele in sorted(rm, reverse = True):  \n",
        "        del row[ele] # remove element from row\n",
        "    \n",
        "    data.append(row[:-1]); # rest of the data as attributes\n",
        "  \n",
        "  return [data, labels];\n",
        "\n",
        "def timeDiff(start, end):\n",
        "  return \"{t:.3f} ms\".format(t = (end-start))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfdZNlmWwPn4"
      },
      "source": [
        "# Loading Data\n",
        "\n",
        "Since all the data preprocessing parts are ready, now it's time to load the datasets into separate variables.\n",
        "\n",
        "All CSV files are uploaded to the repository, and `pandas` will read them directly using the URLs. Then it will assign them to variables after parsing through the `labelSplitter` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKRTABJ1e3Tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a55676-bcbb-48fe-d82f-179a18fd2687"
      },
      "source": [
        "## Training Data\n",
        "splitted = labelSplitter(pd.read_csv(training_dataset_url, header=None))\n",
        "training_data = splitted[0]\n",
        "training_data_labels = splitted[1]\n",
        "\n",
        "## Validation Data\n",
        "splitted = labelSplitter(pd.read_csv(validation_dataset_url, header=None))\n",
        "validation_data = splitted[0]\n",
        "validation_data_labels = splitted[1]\n",
        "\n",
        "## Test Data\n",
        "splitted = labelSplitter(pd.read_csv(test_dataset_url, header=None))\n",
        "test_data = splitted[0]\n",
        "test_data_labels = splitted[1]\n",
        "\n",
        "splitted = None\n",
        "total = len(training_data_labels) + len(validation_data_labels) + len(test_data_labels)\n",
        "\n",
        "print(\"Training dataset size: \", len(training_data_labels))\n",
        "print(\"Validation dataset size: \", len(validation_data_labels))\n",
        "print(\"Testing dataset size: \", len(test_data_labels))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset size:  8444\n",
            "Validation dataset size:  2111\n",
            "Testing dataset size:  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SakDpMXEVQmu"
      },
      "source": [
        "# Training and Validation\n",
        "\n",
        "---\n",
        "\n",
        "It is clear that this is a supervised learning task.\n",
        "\n",
        "**Model Training**\n",
        "\n",
        "SciKit-Learn’s DecisionTreeClassifier is being used as the model. The splitting criterion is set to `Entropy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bBvzYiYqQZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c42f54a-8c94-4a57-e39e-87a217a05a44"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_leaf_nodes=30)\n",
        "\n",
        "start = time.time()*1000\n",
        "clf = clf.fit(training_data, training_data_labels) # train\n",
        "end = time.time()*1000\n",
        "\n",
        "print(\"Training completed in\", timeDiff(start, end))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training completed in 42.658 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_v0q5JsCkc"
      },
      "source": [
        "**Performance Evaluation**\n",
        "\n",
        "Let's measure the accuracy of the the trained model using the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4KmKnfsMkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8ddafc-0048-4a55-b04c-d62419c44571"
      },
      "source": [
        "print(\"Accuracy:\", clf.score(validation_data, validation_data_labels))\n",
        "print(\"Accuracy:\", clf.score(test_data, test_data_labels))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9133112269066793\n",
            "Accuracy: 0.926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so9-kfwCAYfg"
      },
      "source": [
        "dot_data = tree.export_graphviz(clf, out_file=None,\n",
        "                                feature_names=attributes,\n",
        "                                class_names=labels,\n",
        "                                filled=True, rounded=True, special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow2yFBylwUMt"
      },
      "source": [
        "# Testing Model\n",
        "\n",
        "---\n",
        "\n",
        "Below is an interactive system built for testing the trained model by giving testing data as inputs and making predictions in real-time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEGW-jYJwrfi"
      },
      "source": [
        "while(True):\n",
        "  print()\n",
        "  print(\"There are\", len(test_data_labels), \"instances which can be tested.\")\n",
        "  print()\n",
        "\n",
        "  row = int(input(\"Enter row index (-1 exit): \"))\n",
        "\n",
        "  if(row < 0):\n",
        "    break\n",
        "  elif(row >= len(test_data_labels)):\n",
        "    print(\"Error: Enter a number between 0 and\", (len(test_data_labels)-1))\n",
        "    continue\n",
        "\n",
        "  i = []\n",
        "  i.append(test_data[row])\n",
        "\n",
        "  start = time.time()*1000\n",
        "  prediction = clf.predict(i)[0]\n",
        "  end = time.time()*1000\n",
        "\n",
        "  testLbl = labels[0] if (test_data_labels[row] == -1) else labels[1]\n",
        "  predLbl = labels[0] if (prediction == -1) else labels[1]\n",
        "\n",
        "  print()\n",
        "  print(\"Predicted answer:\", prediction, predLbl)\n",
        "  print(\"Correct answer:\", test_data_labels[row], testLbl)\n",
        "  print()\n",
        "\n",
        "  state = \"Incorrect!\"\n",
        "  if (prediction == test_data_labels[row]):\n",
        "    state = \"Correct!\"\n",
        "  print(\"Prediction is\", state, \"(\", timeDiff(start, end) , \")\")\n",
        "  print(\"***\")\n",
        "  print()\n",
        "\n",
        "print()\n",
        "print(\"Application terminated.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}